---
title: "Stats506_PS3"
format:
  html:
    embed-resources: true
editor: visual
---

# Problem 1 - Vision

### Part A:

I will download the VIX_D file and DEMO_D file. I will merge the two files to create a single Stata dataset, using the SEQN variable for merging. I will keep only records which match. Below I will print my total size, showing that it is now 6,980.

```         
```

### Part B:

Without fitting any models, I will estimate the proportion of respondents within each 10-year age bracket (eg. 0-9, 10-19, 20-29, etc) who wear glasses/contact lenses for distance vision. I will produce a nice table with the results below. (There was hint)

```         
```

### Part C:

Fit three logistic regression models predicting whether a respondent wears glasses/contact lenses for distance vision. Predictors:

1.  age
2.  age, race, gender
3.  age, race, gender, poverty income ratio

Next, I will produce a table presenting the estimated odds ratios for the coefficients in each model along with the sample size for the model, the pseud-R\^2, and AIC values.

```         
```

### Part D:

From the third model from the previous part, I will discuss whether the *odds* of men and women being wears of glasses/contact lenses for distance vision differ. I will test whether the *proportion* of wearers of glasses/contact lenses for distance vision differs between men and women. I will include the results of the test and its interpretation.

```         
```

# Problem 2 - Sakila

Load the "sakila" database discussed in class into SQLite. It can be downloaded from the link provided.

```{r}
library(DBI)
library(RSQLite)
```

```{r}
#Load the "sakila" database into SQLite
sakila <- dbConnect(RSQLite::SQLite(), "/Users/19892/OneDrive/Documents/STATS506/ProblemSets/Stats506_PS3/sakila_master.db")
```

### Part A:

Aside from English, what language is most common for films? Answer this with a a single SQL query.

To get a feel for the data base, here are the table names

```{r}
dbListTables(sakila)
```

Below are the languages:

```{r}
dbListFields(sakila, "language")
dbGetQuery(sakila, "SELECT name FROM language")
dbGetQuery(sakila, "SELECT language_id from language")
```

```{r}
dbGetQuery(sakila,"
  SELECT language.name AS most_common_language, COUNT(*) AS film_count
    FROM film
      INNER JOIN language ON film.language_id = language.language_id
    WHERE language.name != 'English'
    GROUP BY language.name
    ORDER BY film_count DESC
    LIMIT 1
           "
           )
```

Above we get that there are no films with the language being something other than English. Therefore, English is the only language being used in this database and therefore is the most popular

*For each of the following questions, solve them in two ways: First, use SQL query or queries to extract the appropriate table(s), then use regular R to answer the question. Second, use a single SQL query to answer the question.*

### Part B: What genre of movie is the most common in the data, and how many movies are of this genre?

SQL:

Multiple Queries: First I will print out all the genres to get a feel with what options we have below:

```{r}
dbListFields(sakila, "film_category")
dbGetQuery(sakila, "SELECT name FROM category")
```

```{r}
count <- dbGetQuery(sakila,"
  SELECT category.name AS genre_name, COUNT(*) AS genre_count
    FROM film
      INNER JOIN film_category ON film.film_id = film_category.film_id
      INNER JOIN category ON film_category.category_id = category.category_id
  GROUP BY category.name
           "
           )
```

Join the the tables of film, film_category, and category to count the number of movies in each genre using genre_count and group by the cateogry.name to get the resutling table below.

Below is the resulting table from the above query and I will find the max to get the most popular genre:

```{r}
count
count[which.max(count$genre_count),]
```

Above, we can see that the category_id of the most popular genre of the films in this database is sports with 74 movies that have the genre.

Below, I will answer the same question with a single query:

```{r}
dbGetQuery(sakila, "
  SELECT category.name AS most_common_genre, COUNT(*) AS movie_count
    FROM film
      INNER JOIN film_category ON film.film_id = film_category.film_id
      INNER JOIN category ON film_category.category_id = category.category_id
    GROUP BY category.name
    ORDER BY movie_count DESC
    LIMIT 1
")
```

We get the name of the genre with the category table using most_common_genre. We count the number of movies in each genre using the film table along with the film_category and category tables and grouping the results using GROUP BY. Order the movie counts using ORDER BY, LIMIT 1 gets the top genre count.

Above, in a single SQL query, I have also found that the most popular genre of film is sports with 74 movies.

### Part C: Identify which country or countries have exactly 9 customers. Answer this with a single SQL query. 

Below I will solve the question using the first way described: Using SQL query or queries to extract the appropriate table(s), then use regular R to answer the question.

```{r}
customer_data <- dbGetQuery(sakila, "
  SELECT country.country, COUNT(*) AS customer_count
    FROM country
        INNER JOIN city ON country.country_id = city.country_id
        INNER JOIN address ON city.city_id = address.city_id
        INNER JOIN customer ON address.address_id = customer.address_id
    GROUP BY country.country
                            ")
```

Print the customer_data table that the above query achieves below and answer the question, which country has a customer_count of 9:

```{r}
customer_data
```

```{r}
nine_customers <- customer_data[customer_data$customer_count ==9, ]
nine_customers
```

Above we can see that the one country that has a customer_count of 9 is the United Kingdom.

Below I will answer the question with a single SQL query:

```{r}
dbGetQuery(sakila, "
  SELECT country.country, COUNT(*) AS customer_count 
    FROM country
      INNER JOIN city ON country.country_id = city.country_id
      INNER JOIN address ON city.city_id = address.city_id
      INNER JOIN customer ON address.address_id = customer.address_id
    GROUP BY country.country
    HAVING customer_count = 9
           ")
```

We use customer_count to count how many customers are in each country. We do this using a series of INNER JOIN shown above. As a result, we get that the United Kingdom was the only country with a customer count of 9.

# Problem 3 - US Records

Download the "US - 500 Records" data from the link provided and import it into R below. (This is fake data) and answer the question below:

Import Data Below:

```{r}
library(tidyverse)
us_500 <- read_csv('/Users/19892/OneDrive/Documents/STATS506/ProblemSets/Stats506_PS3/us-500.csv')
```

Below is the first several rows of the data set to get an idea with what we're working with:

```{r}
head(us_500)
```

### Part A: What proportion of email addresses are hosted at a domain with TLD ".net"? (E.g. in the email, "angrycat\@freemail.org", "freemail.org" is the domain, with TLD (top-level domain) ".org".)

```{r}
library(urltools)
tlds <- tld_extract(us_500$email)
tld_with_net <- tlds[which(tlds$tld == "net"),]
prop <- nrow(tld_with_net)/nrow(tlds)
paste("Proportion of email addresses hosted at a domain with TLD .net:", prop)
```

Above, I used the tld_extract function from urltools (found in this link https://search.r-project.org/CRAN/refmans/urltools/html/tld_extract.html) to get the tlds from each email and the which() function to extract all tlds with ".net" to finally get the proportion of 0.14.

### Part B: What proportion of email addresses have at least one non alphanumeric character in them?

```{r}
#' Title: Fucntion to take in an email and return a TRUE/FALSE depending on if it has an non-alphanumeric character or not.
#'
#' @param email is the given email the function is checking.
#'
#' @return TRUE/FALSE, TRUE if the email does contain at least one non-alpha numeric characters, FALSE if they do not. 
#' @export
#'
#' @examples
non_alphanum <- function(email){
  email <- gsub("[@.]", "", email)
  grepl("[^A-Za-z0-9]", email)
}
```

Above, gsub() substitutes @ or . with nothing to make sure the grepl() function does not identify those as non-aphanumeric characters. grepl() checks if there is a non- alphanumeric character in the email.

```{r}
non_alphanum_emails <- sapply(us_500$email, non_alphanum)
prop <- mean(non_alphanum_emails)
prop
```

Above we apply the function using sapply() on the emails in the data and return the proportion which is 0.248 of characters that have non-alphanumeric characters in them.

### Part C: What is the most common area code amonst all phone numbers?

Below I will use the str_extract() function from the stringr package (found in https://stackoverflow.com/questions/64580834/extracting-area-codes-using-extract-on-r)

```{r}
library(stringr)
area_codes <- str_extract(us_500$phone1, "\\d{3}")
area_codes_all <- append(area_codes, str_extract(us_500$phone2, "\\d{3}"))

sort(table(area_codes_all), decreasing = TRUE)[1]
```

Above, get a list of all the area codes from both the phone1 and phone2 variables (area_codes_all). I get a table of all the area codes and their counts and sort them with the highest count at the top. Therefore, we get that the most common area code is (973) shown above.

### Part D: Produce a histogram of the log of the apartment numbers for all addresses. (You may assume any number after the street is an apartment number.)

Below I will extract the log of the apartment numbers by first extracting the apartment number from the address variable using gsub() and takes any number after \# and then taking the log of each number. I will then generate a histogram.

```{r}
apartment_numbers <- gsub(".*#(\\d+).*|.*", "\\1", us_500$address)
apartment_numbers <- as.numeric(apartment_numbers)
log_apartment_numbers <- log(apartment_numbers)

#Generate Historgram
hist(log_apartment_numbers, main = "Histogram of Log(Apartment Numbers)",
     xlab = "Log(Apartment Numbers)", ylab = "Frequency")
```

Above is a histogram of the log of the apartment numbers for all of the addresses.

### Part E: Benford's law is an observation about the distribution of the leading digit of real numerical data. Examine whether the apartment numbers appear to follow Benford's law. Do you thing the apartment numbers would pass as real data? 

Upon looking at the link for Benford's law the Probability of the leading digit:

log10(1+1/d)

Where d is the leading digit of the given number, in the case of this question, the apartment number.

```{r}
# Extract the leading digit
leading_digits <- as.numeric(substr(apartment_numbers, 1, 1))

# Calculate the observed frequencies
observed_frequencies <- table(leading_digits)
observed_frequencies <- observed_frequencies/sum(observed_frequencies)
observed_frequencies

# Calculate the expected frequencies according to Benford's Law
expected_freq <- log10(1 + 1/(1:9))
expected_freq
```

As we can see above, the leading digits of the apartment numbers do not satisfy benford's law because the frequencies do not match up. In fact, the leading digit frequency in the data is pretty even between all numbers. Therefore, because Benford's law is said to be an observation on many real-life sets of numberical data, I would say the numbers would not pass as real data.

### Part F: Repeat your analysis of Benford's law on the *last* digit of the street number (E.g. if your address is "123 Main St #25", your street number is "123".)

Below, I will do something similar to part (e) except instead of extracting the leading digits of the aparment numbers I will extract the last digit of the street numbers and compare it to the same Benford's law distribution:

```{r}
st_num <- gsub("^(\\d+).*", "\\1", us_500$address)
#get the last digit using the nchar() function to find the length of each street number:
last_digits <- substr(st_num, nchar(st_num), nchar(st_num))
#Convert into a table:
last_digits <- table(last_digits)
#Benford's law is only on numbers 1-9 so take out 0 column:
last_digits <- last_digits[2:10]
last_digits_freq <- last_digits/sum(last_digits)
last_digits_freq
expected_freq
```

Again, similar to part (e) we can observe that the frequencies of the last digit are about the same through all numbers 1-9 so they do not follow Benford's Law. Because of the same reason stated in part (e) these numbers would not pass as real data.
